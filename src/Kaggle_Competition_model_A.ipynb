{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53897189",
   "metadata": {},
   "source": [
    "### 1. Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77dad5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import category_encoders as ce\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1881d77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: category_encoders in /home/buwu/.local/lib/python3.9/site-packages (2.6.1)\n",
      "Requirement already satisfied: patsy>=0.5.1 in /opt/conda/lib/python3.9/site-packages (from category_encoders) (0.5.1)\n",
      "Requirement already satisfied: statsmodels>=0.9.0 in /opt/conda/lib/python3.9/site-packages (from category_encoders) (0.12.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.9/site-packages (from category_encoders) (1.21.1)\n",
      "Requirement already satisfied: pandas>=1.0.5 in /opt/conda/lib/python3.9/site-packages (from category_encoders) (1.5.3)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /opt/conda/lib/python3.9/site-packages (from category_encoders) (0.24.2)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from category_encoders) (1.7.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.9/site-packages (from pandas>=1.0.5->category_encoders) (2021.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.9/site-packages (from pandas>=1.0.5->category_encoders) (2.8.2)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.9/site-packages (from patsy>=0.5.1->category_encoders) (1.16.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.20.0->category_encoders) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.9/site-packages (from scikit-learn>=0.20.0->category_encoders) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install category_encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a30a6ae",
   "metadata": {},
   "source": [
    "### 2. Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5c3ff45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRIP_ID</th>\n",
       "      <th>CALL_TYPE</th>\n",
       "      <th>ORIGIN_CALL</th>\n",
       "      <th>ORIGIN_STAND</th>\n",
       "      <th>TAXI_ID</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>DAY_TYPE</th>\n",
       "      <th>MISSING_DATA</th>\n",
       "      <th>POLYLINE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1372636858620000589</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000589</td>\n",
       "      <td>1372636858</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.618643,41.141412],[-8.618499,41.141376],[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1372637303620000596</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20000596</td>\n",
       "      <td>1372637303</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.639847,41.159826],[-8.640351,41.159871],[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1372636951620000320</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000320</td>\n",
       "      <td>1372636951</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.612964,41.140359],[-8.613378,41.14035],[-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1372636854620000520</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000520</td>\n",
       "      <td>1372636854</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.574678,41.151951],[-8.574705,41.151942],[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1372637091620000337</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000337</td>\n",
       "      <td>1372637091</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.645994,41.18049],[-8.645949,41.180517],[-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               TRIP_ID CALL_TYPE  ORIGIN_CALL  ORIGIN_STAND   TAXI_ID  \\\n",
       "0  1372636858620000589         C          NaN           NaN  20000589   \n",
       "1  1372637303620000596         B          NaN           7.0  20000596   \n",
       "2  1372636951620000320         C          NaN           NaN  20000320   \n",
       "3  1372636854620000520         C          NaN           NaN  20000520   \n",
       "4  1372637091620000337         C          NaN           NaN  20000337   \n",
       "\n",
       "    TIMESTAMP DAY_TYPE  MISSING_DATA  \\\n",
       "0  1372636858        A         False   \n",
       "1  1372637303        A         False   \n",
       "2  1372636951        A         False   \n",
       "3  1372636854        A         False   \n",
       "4  1372637091        A         False   \n",
       "\n",
       "                                            POLYLINE  \n",
       "0  [[-8.618643,41.141412],[-8.618499,41.141376],[...  \n",
       "1  [[-8.639847,41.159826],[-8.640351,41.159871],[...  \n",
       "2  [[-8.612964,41.140359],[-8.613378,41.14035],[-...  \n",
       "3  [[-8.574678,41.151951],[-8.574705,41.151942],[...  \n",
       "4  [[-8.645994,41.18049],[-8.645949,41.180517],[-...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tr = pd.read_csv(\"../data/train.csv\")\n",
    "df_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d08ade",
   "metadata": {},
   "source": [
    "### 3. Get Computed Time from POLYLINE\n",
    "\n",
    "Our goal is to predict the travel-time of the taxi, which can be derived from the POLYLINE length.\n",
    "\n",
    "Recall:\n",
    "\n",
    "```\n",
    "The travel time of the trip (the prediction target of this project) is defined as the (number of points-1) x 15 seconds. \n",
    "For example, a trip with 101 data points in POLYLINE has a length of (101-1) * 15 = 1500 seconds. Some trips have missing data points in POLYLINE, indicated by MISSING_DATA column, and it is part of the challenge how you utilize this knowledge.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "137aba93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over every single \n",
    "def polyline_to_trip_duration(polyline):\n",
    "  return max(polyline.count(\"[\") - 1, 0) * 15\n",
    "\n",
    "# This code creates a new column, \"LEN\", in our dataframe. The value is\n",
    "# the (polyline_length - 1) * 15\n",
    "df_tr[\"LEN\"] = df_tr[\"POLYLINE\"].apply(polyline_to_trip_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c16cb37",
   "metadata": {},
   "source": [
    "### 4. Test and Extract the features: (Original Call + HR + WK + MON + TAXI_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15b58b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern is found!\n"
     ]
    }
   ],
   "source": [
    "# Verify our guesses of the patterns of TAXI_ID such that all the IDs are in the form of \n",
    "# 20000xxx by substracting all the numbers by 20000000 and check if they are between the \n",
    "# range [0,1000).\n",
    "def TAXI_ID_pattern_checker(x):\n",
    "    # Test if the only last 3 digits of the TRIP_ID exhibit a pattern\n",
    "    for idx in range(len(x)):\n",
    "        if (x[idx]-20000000) < 0 or (x[idx]-20000000) >= 1000:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "if TAXI_ID_pattern_checker(df_tr[\"TAXI_ID\"]):\n",
    "    print(\"Pattern is found!\")\n",
    "\n",
    "# Note that the only last three digits of the TAXI_ID are nonzero.\n",
    "def parse_TAXI_ID(x):\n",
    "    return (x % pow(10,3)) \n",
    "\n",
    "df_tr[\"Unique_TAXI_ID\"] = df_tr[\"TAXI_ID\"].apply(parse_TAXI_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6abbca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def parse_time(x):\n",
    "  # We are using python's builtin datetime library\n",
    "  # https://docs.python.org/3/library/datetime.html#datetime.date.fromtimestamp\n",
    "\n",
    "  # Each x is essentially a 1 row, 1 column pandas Series\n",
    "  dt = datetime.fromtimestamp(x[\"TIMESTAMP\"])\n",
    "  return dt.year, dt.month, dt.day, dt.hour, dt.weekday(), dt.minute\n",
    "\n",
    "# Because we are assigning multiple values at a time, we need to \"expand\" our computed (year, month, day, hour, weekday) tuples on \n",
    "# the column axis, or axis 1\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html\n",
    "\n",
    "df_tr[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\", \"MIN\"]] = df_tr[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76ba7c2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outlier_threshold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_763/512821004.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# This is to remove outliers. Otherwise, our plots would look very squished (since there are some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# VERRRRRY long taxi trips in the dataset)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf_trimmed_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_A\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype_A\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"LEN\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmean\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutlier_threshold\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;31m# df_trimmed_A = df_trimmed_A[df_trimmed_A?[\"ORIGIN_CALL\"]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# plt.scatter(df_trimmed_A[\"ORIGIN_CALL\"], df_trimmed_A[\"LEN\"],s=5, alpha=0.5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'outlier_threshold' is not defined"
     ]
    }
   ],
   "source": [
    "# Trying to find relationship between \"ORIGIN_CALL\" and \"LEN\"\n",
    "type_A = df_tr[df_tr[\"CALL_TYPE\"]==\"A\"]\n",
    "\n",
    "mean, std = type_A[\"LEN\"].mean(), type_A[\"LEN\"].std()\n",
    "# \"Choose all data, where the trip length is less than 3 standard deviations away from the mean\"\n",
    "# This is to remove outliers. Otherwise, our plots would look very squished (since there are some\n",
    "# VERRRRRY long taxi trips in the dataset)\n",
    "df_trimmed_A = type_A[type_A[\"LEN\"] < mean + outlier_threshold * std]\n",
    "# df_trimmed_A = df_trimmed_A[df_trimmed_A?[\"ORIGIN_CALL\"]\n",
    "# plt.scatter(df_trimmed_A[\"ORIGIN_CALL\"], df_trimmed_A[\"LEN\"],s=5, alpha=0.5)\n",
    "# plt.xlabel(\"ORIGIN_CALL\")\n",
    "# plt.ylabel(\"LEN\")\n",
    "# plt.title(\"Relationship between ORIGIN_CALL and LEN\")\n",
    "# plt.show()\n",
    "\n",
    "print(\"The correlation coefficient between ORIGIN CALL and LEN is {}\".format(df_trimmed_A[\"ORIGIN_CALL\"].corr(df_trimmed_A[\"LEN\"])))\n",
    "print(\"The correlation coefficient between HR and LEN is {}\".format(df_trimmed_A[\"HR\"].corr(df_trimmed_A[\"LEN\"])))\n",
    "print(\"The correlation coefficient between WK and LEN is {}\".format(df_trimmed_A[\"WK\"].corr(df_trimmed_A[\"LEN\"])))\n",
    "print(\"The correlation coefficient between MON and LEN is {}\".format(df_trimmed_A[\"MON\"].corr(df_trimmed_A[\"LEN\"])))\n",
    "print(\"The correlation coefficient between TAXI_ID and LEN is {}\".format(df_trimmed_A[\"Unique_TAXI_ID\"].corr(df_trimmed_A[\"LEN\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a19787c",
   "metadata": {},
   "source": [
    "### 5. Data Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e8c0452",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预处理之后数据形状: (360422, 24)\n",
      "         ORIGIN_CALL_0  ORIGIN_CALL_1  ORIGIN_CALL_2  ORIGIN_CALL_3  \\\n",
      "12                   0              0              0              0   \n",
      "17                   0              0              0              0   \n",
      "21                   0              0              0              0   \n",
      "24                   0              0              0              0   \n",
      "26                   0              0              0              0   \n",
      "...                ...            ...            ...            ...   \n",
      "1710651              0              0              0              0   \n",
      "1710653              0              0              1              0   \n",
      "1710656              0              0              0              1   \n",
      "1710658              0              0              0              0   \n",
      "1710659              0              1              1              1   \n",
      "\n",
      "         ORIGIN_CALL_4  ORIGIN_CALL_5  ORIGIN_CALL_6  ORIGIN_CALL_7  \\\n",
      "12                   0              0              0              0   \n",
      "17                   0              0              0              0   \n",
      "21                   0              0              0              0   \n",
      "24                   0              0              0              0   \n",
      "26                   0              0              0              0   \n",
      "...                ...            ...            ...            ...   \n",
      "1710651              0              0              0              0   \n",
      "1710653              0              0              0              1   \n",
      "1710656              0              1              1              1   \n",
      "1710658              0              0              0              0   \n",
      "1710659              1              1              0              0   \n",
      "\n",
      "         ORIGIN_CALL_8  ORIGIN_CALL_9  ...  ORIGIN_CALL_14  ORIGIN_CALL_15  \\\n",
      "12                   0              0  ...               0               1   \n",
      "17                   0              0  ...               1               0   \n",
      "21                   0              0  ...               1               1   \n",
      "24                   0              0  ...               0               0   \n",
      "26                   0              0  ...               0               1   \n",
      "...                ...            ...  ...             ...             ...   \n",
      "1710651              0              0  ...               1               1   \n",
      "1710653              1              1  ...               0               1   \n",
      "1710656              0              0  ...               0               1   \n",
      "1710658              0              0  ...               1               1   \n",
      "1710659              0              1  ...               1               1   \n",
      "\n",
      "         HR_0  HR_1  HR_2  HR_3  HR_4  WK_0  WK_1  WK_2  \n",
      "12          0     0     0     0     1     0     0     1  \n",
      "17          0     0     0     0     1     0     0     1  \n",
      "21          0     0     0     0     1     0     0     1  \n",
      "24          0     0     0     0     1     0     0     1  \n",
      "26          0     0     0     0     1     0     0     1  \n",
      "...       ...   ...   ...   ...   ...   ...   ...   ...  \n",
      "1710651     1     0     0     1     1     0     0     1  \n",
      "1710653     0     0     1     1     0     1     1     0  \n",
      "1710656     0     0     1     0     1     1     1     1  \n",
      "1710658     0     1     1     1     1     0     1     0  \n",
      "1710659     0     1     0     0     0     1     1     0  \n",
      "\n",
      "[360422 rows x 24 columns]\n",
      "['ORIGIN_CALL_0', 'ORIGIN_CALL_1', 'ORIGIN_CALL_2', 'ORIGIN_CALL_3', 'ORIGIN_CALL_4', 'ORIGIN_CALL_5', 'ORIGIN_CALL_6', 'ORIGIN_CALL_7', 'ORIGIN_CALL_8', 'ORIGIN_CALL_9', 'ORIGIN_CALL_10', 'ORIGIN_CALL_11', 'ORIGIN_CALL_12', 'ORIGIN_CALL_13', 'ORIGIN_CALL_14', 'ORIGIN_CALL_15', 'HR_0', 'HR_1', 'HR_2', 'HR_3', 'HR_4', 'WK_0', 'WK_1', 'WK_2']\n"
     ]
    }
   ],
   "source": [
    "outlier_threshold = 3\n",
    "\n",
    "type_A = df_tr[df_tr[\"CALL_TYPE\"]==\"A\"]\n",
    "\n",
    "mean, std = type_A[\"LEN\"].mean(), type_A[\"LEN\"].std()\n",
    "# \"Choose all data, where the trip length is less than 3 standard deviations away from the mean\"\n",
    "# This is to remove outliers. Otherwise, our plots would look very squished (since there are some\n",
    "# VERRRRRY long taxi trips in the dataset)\n",
    "df_trimmed_A = type_A[type_A[\"LEN\"] < mean + outlier_threshold * std]\n",
    "\n",
    "# print(type_A[\"ORIGIN_CALL\"].mean())\n",
    "# print(type_A[\"ORIGIN_CALL\"].std())\n",
    "\n",
    "all_features = df_trimmed_A[[]]\n",
    "\n",
    "# Create an instance of BinaryEncoder\n",
    "binary_encoder_origin = ce.BinaryEncoder(cols=['ORIGIN_CALL'])\n",
    "binary_encoder_hr = ce.BinaryEncoder(cols=['HR'])\n",
    "binary_encoder_wk = ce.BinaryEncoder(cols=['WK'])\n",
    "\n",
    "# Apply binary encoding to the 'ORIGIN_CALL' column\n",
    "encoded_data_origin = binary_encoder_origin.fit_transform(df_trimmed_A['ORIGIN_CALL'])\n",
    "encoded_data_hr = binary_encoder_hr.fit_transform(df_trimmed_A['HR'])\n",
    "encoded_data_wk = binary_encoder_wk.fit_transform(df_trimmed_A['WK'])\n",
    "\n",
    "# Convert all features into string so that we can apply pd.get_dummies on it\n",
    "# all_features['WK'] = all_features['WK'].astype(str)\n",
    "# all_features['HR'] = all_features['HR'].astype(str)\n",
    "\n",
    "\n",
    "# numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index\n",
    "# all_features[numeric_features] = all_features[numeric_features].apply(\n",
    "#     lambda x: (x - x.mean()) / (x.std()))\n",
    "\n",
    "# all_features = pd.get_dummies(all_features)\n",
    "\n",
    "# Concatenate the encoded data with the original DataFrame\n",
    "all_features = pd.concat([all_features, encoded_data_origin], axis=1)\n",
    "all_features = pd.concat([all_features, encoded_data_hr], axis=1)\n",
    "all_features = pd.concat([all_features, encoded_data_wk], axis=1)\n",
    "# all_features = pd.concat([all_features, encoded_data_mon], axis=1)\n",
    "\n",
    "\n",
    "print(f'预处理之后数据形状: {all_features.shape}')\n",
    "print(all_features)\n",
    "\n",
    "cols = list(all_features.columns.values)\n",
    "print(cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1889d057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数据: torch.Size([288337, 24])\n",
      "训练集label: torch.Size([288337, 1])\n",
      "验证集数据: torch.Size([72085, 24])\n",
      "验证集label: torch.Size([72085, 1])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "label = df_trimmed_A[\"LEN\"]\n",
    "train_data = all_features\n",
    "\n",
    "train_features, val_features, train_labels, val_labels = train_test_split(\n",
    "    train_data, label, test_size=0.2, random_state=42)\n",
    "\n",
    "# 训练集\n",
    "train_features = torch.tensor(train_features.values, dtype=torch.float)\n",
    "# 验证集\n",
    "val_features = torch.tensor(val_features.values, dtype=torch.float)\n",
    "\n",
    "train_labels = torch.tensor(train_labels.values, dtype=torch.float)\n",
    "train_labels = train_labels.unsqueeze(1) \n",
    "\n",
    "\n",
    "val_labels = torch.tensor(val_labels.values, dtype=torch.float)\n",
    "val_labels = val_labels.unsqueeze(1)\n",
    "\n",
    "\n",
    "print(f'训练集数据: {train_features.shape}')\n",
    "print(f'训练集label: {train_labels.shape}')\n",
    "print(f'验证集数据: {val_features.shape}')\n",
    "print(f'验证集label: {val_labels.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0e291ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataset:\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx, :], self.label[idx]\n",
    "\n",
    "train_dataset = myDataset(train_features, train_labels)\n",
    "val_dataset = myDataset(val_features, val_labels)\n",
    "\n",
    "# 变为迭代器\n",
    "train_iter = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=64, shuffle=True, num_workers=4)\n",
    "val_iter = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=64, shuffle=False, num_workers=4)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d892fe6a",
   "metadata": {},
   "source": [
    "### 6. Conversion of Dataset to Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5266f438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 初始化权重\n",
    "def _weight_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "    elif isinstance(m, nn.BatchNorm1d):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "# 网络\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(24, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc5 = nn.Linear(128, 64)  # New layer: fc5 -> fc6\n",
    "        self.fc6 = nn.Linear(64, 32)   # New layer: fc6 -> fc7\n",
    "        self.fc7 = nn.Linear(32, 1)    # New layer: fc7 -> output\n",
    "        self.apply(_weight_init)\n",
    "        self.apply(_weight_init) # 初始化参数\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = self.fc7(x)\n",
    "        return x\n",
    "\n",
    "# 使用rmse作为自定义得分函数，这也是比赛的判定标准\n",
    "def custom_score(y_true, y_pred):\n",
    "#     rmse = mean_squared_error(np.log1p(y_true), np.log1p(y_pred), squared=False)\n",
    "    return math.sqrt(np.mean((np.array(y_pred)-np.array(y_true))*(np.array(y_pred)-np.array(y_true))))\n",
    "\n",
    "net = Net()\n",
    "criterion = torch.nn.MSELoss() # 损失函数为MSE\n",
    "net = net.to(device) # 将网络和损失函数转化为GPU或CPU\n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.Adam(params=net.parameters(), lr=0.001, weight_decay=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993ea7b3",
   "metadata": {},
   "source": [
    "### 7. Model training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f17783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这是训练函数，分为train和val\n",
    "# train时前向传播后向更新参数\n",
    "# val时只计算损失函数\n",
    "def train(net, data_iter, phase, criterion, optimizer=None):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    mean_loss = []\n",
    "    is_grad = True if phase == 'train' else False\n",
    "    with torch.set_grad_enabled(is_grad):\n",
    "        net.train()\n",
    "        for step, (X, y) in enumerate(data_iter):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            out = net(X)\n",
    "            loss = criterion(out, y) # 计算损失\n",
    "            mean_loss.append(loss.item())\n",
    "            \n",
    "            if phase == 'train':\n",
    "                optimizer.zero_grad() # optimizer 0\n",
    "                loss.backward() # back propragation\n",
    "                optimizer.step() # update the paramters\n",
    "\n",
    "            # 将每一个step的结果加入列表，最后统一生产这一个epoch的指标  \n",
    "            # 添加预测值和真实类标签\n",
    "            y_pred.extend(out.detach().cpu().squeeze().numpy().tolist())\n",
    "            y_true.extend(y.detach().cpu().squeeze().numpy().tolist())\n",
    "\n",
    "    # 全量样本的rmse和平均loss\n",
    "    rmse = custom_score(y_true, y_pred)\n",
    "    mean_loss = np.mean(mean_loss)\n",
    "    # 保留4位小数\n",
    "    rmse = np.round(rmse, 4)\n",
    "    mean_loss = np.round(mean_loss, 4)\n",
    "    return mean_loss, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b4f9dd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-24 05:57:05.177620 开始训练结束...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [00:38<12:13, 38.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[40mEpoch: 0 Train loss: 137393.7854 Val loss: 132943.3197 Train score: 370.6716 Val score: 364.6035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [01:16<11:22, 37.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[40mEpoch: 1 Train loss: 132129.3673 Val loss: 130229.5046 Train score: 363.5059 Val score: 360.8925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [01:56<11:02, 38.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[40mEpoch: 2 Train loss: 130983.4608 Val loss: 128785.4369 Train score: 361.9143 Val score: 358.8689\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [02:36<10:32, 39.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[40mEpoch: 3 Train loss: 129942.8082 Val loss: 128151.618 Train score: 360.486 Val score: 357.9833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [03:17<09:57, 39.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[40mEpoch: 4 Train loss: 128951.4759 Val loss: 127249.6522 Train score: 359.0986 Val score: 356.7325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [03:57<09:20, 40.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[40mEpoch: 5 Train loss: 128164.2291 Val loss: 126584.4988 Train score: 358.0107 Val score: 355.7879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [04:37<08:40, 40.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[40mEpoch: 6 Train loss: 127434.1917 Val loss: 126147.3719 Train score: 356.9815 Val score: 355.1816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [05:17<08:01, 40.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[40mEpoch: 7 Train loss: 126704.3725 Val loss: 125946.9352 Train score: 355.9467 Val score: 354.8919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [05:57<07:20, 40.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[40mEpoch: 8 Train loss: 125977.3948 Val loss: 124348.1749 Train score: 354.9458 Val score: 352.6089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [06:41<06:50, 41.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[40mEpoch: 9 Train loss: 125190.7924 Val loss: 124419.4175 Train score: 353.8288 Val score: 352.7377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [07:25<06:17, 41.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[40mEpoch: 10 Train loss: 124504.149 Val loss: 122404.9915 Train score: 352.8554 Val score: 349.8551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [08:10<05:44, 43.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[40mEpoch: 11 Train loss: 123798.5433 Val loss: 122760.2858 Train score: 351.8407 Val score: 350.3853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [08:55<05:05, 43.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[40mEpoch: 12 Train loss: 123021.6545 Val loss: 122499.2305 Train score: 350.7522 Val score: 349.9998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [09:40<04:23, 43.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[40mEpoch: 13 Train loss: 122219.515 Val loss: 121847.6542 Train score: 349.6163 Val score: 349.0863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [10:24<03:40, 44.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[40mEpoch: 14 Train loss: 121377.6543 Val loss: 119274.6597 Train score: 348.4106 Val score: 345.3718\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [11:09<02:57, 44.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[40mEpoch: 15 Train loss: 120537.5716 Val loss: 118370.337 Train score: 347.1951 Val score: 344.0599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [11:54<02:13, 44.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[40mEpoch: 16 Train loss: 119853.1826 Val loss: 119143.2205 Train score: 346.2112 Val score: 345.1843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [12:38<01:28, 44.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[40mEpoch: 17 Train loss: 119074.9343 Val loss: 116685.913 Train score: 345.0842 Val score: 341.5795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [13:22<00:44, 44.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[40mEpoch: 18 Train loss: 118274.8149 Val loss: 117469.8591 Train score: 343.9178 Val score: 342.7558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [14:06<00:00, 42.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[40mEpoch: 19 Train loss: 117425.5637 Val loss: 115109.56 Train score: 342.6404 Val score: 339.2509\n",
      "2023-05-24 06:11:11.403460 训练结束...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from colorama import Fore, Back\n",
    "\n",
    "epochs = 100\n",
    "loss_list_A = []\n",
    "print(f'{datetime.now()} 开始训练结束...')\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_mean_loss, train_score = train(net=net, \n",
    "                                         data_iter=train_iter, \n",
    "                                         phase='train', \n",
    "                                         criterion=criterion, \n",
    "                                         optimizer=optimizer)\n",
    "    \n",
    "    val_mean_loss, val_score = train(net=net, \n",
    "                                     data_iter=train_iter, \n",
    "                                     phase='val', \n",
    "                                     criterion=criterion, \n",
    "                                     optimizer=None)\n",
    "    print(Fore.CYAN + Back.BLACK, end='')\n",
    "    tqdm.write(f'Epoch: {epoch} Train loss: {train_mean_loss} Val loss: {val_mean_loss}', end=' ')\n",
    "    tqdm.write(f'Train score: {train_score} Val score: {val_score}')\n",
    "    loss_list_A.append(train_score)\n",
    "\n",
    "print(f'{datetime.now()} 训练结束...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c095eba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('loss_list_A.txt', 'w') as file:\n",
    "    file.write(','.join(str(element) for element in loss_list_A))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbf8453",
   "metadata": {},
   "source": [
    "### 8. Test set validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4078f480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE loss against test set is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "703.9716069295529"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tr_test = pd.read_csv(\"../test/test.csv\")\n",
    "df_tr_test.head()\n",
    "\n",
    "df_tr_test[\"LEN\"] = df_tr_test[\"POLYLINE\"].apply(polyline_to_trip_duration)\n",
    "df_tr_test[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\", \"MIN\"]] = df_tr_test[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    "\n",
    "df_tr_test = df_tr_test[df_tr_test[\"CALL_TYPE\"]==\"A\"]\n",
    "\n",
    "all_features_test = df_tr_test[[]]\n",
    "\n",
    "# Create an instance of BinaryEncoder\n",
    "binary_encoder_origin_test = ce.BinaryEncoder(cols=['ORIGIN_CALL'])\n",
    "binary_encoder_hr_test = ce.BinaryEncoder(cols=['HR'])\n",
    "binary_encoder_wk_test = ce.BinaryEncoder(cols=['WK'])\n",
    "# binary_encoder_mon_test = ce.BinaryEncoder(cols=['MON'])\n",
    "\n",
    "\n",
    "# Apply binary encoding to the 'ORIGIN_CALL' column\n",
    "encoded_data_origin_test = binary_encoder_origin_test.fit_transform(df_tr_test['ORIGIN_CALL'])\n",
    "encoded_data_hr_test = binary_encoder_hr_test.fit_transform(df_tr_test['HR'])\n",
    "encoded_data_wk_test = binary_encoder_wk_test.fit_transform(df_tr_test['WK'])\n",
    "# encoded_data_mon_test = binary_encoder_mon_test.fit_transform(df_tr_test['MON'])\n",
    "\n",
    "# Concatenate the encoded data with the original DataFrame\n",
    "all_features_test = pd.concat([all_features_test, encoded_data_origin_test], axis=1)\n",
    "all_features_test = pd.concat([all_features_test, encoded_data_hr_test], axis=1)\n",
    "all_features_test = pd.concat([all_features_test, encoded_data_wk_test], axis=1)\n",
    "# all_features_test = pd.concat([all_features_test, encoded_data_mon_test], axis=1)\n",
    "\n",
    "missing_columns = set(all_features.columns)-set(all_features_test.columns)\n",
    "\n",
    "for column in missing_columns:\n",
    "    all_features_test[column] = 0\n",
    "\n",
    "# print(f'预处理之后数据形状: {all_features_test.shape}')\n",
    "# print(all_features_test)\n",
    "\n",
    "cols = list(all_features_test.columns.values)\n",
    "# print(cols)\n",
    "\n",
    "prediction = net(torch.tensor(all_features_test.values, dtype=torch.float).to(device))\n",
    "\n",
    "print(\"The RMSE loss against test set is:\")\n",
    "custom_score(prediction.tolist(),df_tr_test[\"LEN\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98808ece",
   "metadata": {},
   "source": [
    "### 9. Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c0c904bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Optimizer's state_dict:\")\n",
    "# for var_name in optimizer.state_dict():\n",
    "#     print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
    "# val_features = val_features.to(device)\n",
    "torch.save(net.state_dict(), '../model/modelA.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f085af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
