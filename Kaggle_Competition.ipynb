{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53897189",
   "metadata": {},
   "source": [
    "### 1. Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "77dad5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a30a6ae",
   "metadata": {},
   "source": [
    "### 2. Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b5c3ff45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRIP_ID</th>\n",
       "      <th>CALL_TYPE</th>\n",
       "      <th>ORIGIN_CALL</th>\n",
       "      <th>ORIGIN_STAND</th>\n",
       "      <th>TAXI_ID</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>DAY_TYPE</th>\n",
       "      <th>MISSING_DATA</th>\n",
       "      <th>POLYLINE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1372636858620000589</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000589</td>\n",
       "      <td>1372636858</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.618643,41.141412],[-8.618499,41.141376],[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1372637303620000596</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20000596</td>\n",
       "      <td>1372637303</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.639847,41.159826],[-8.640351,41.159871],[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1372636951620000320</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000320</td>\n",
       "      <td>1372636951</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.612964,41.140359],[-8.613378,41.14035],[-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1372636854620000520</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000520</td>\n",
       "      <td>1372636854</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.574678,41.151951],[-8.574705,41.151942],[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1372637091620000337</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000337</td>\n",
       "      <td>1372637091</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.645994,41.18049],[-8.645949,41.180517],[-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               TRIP_ID CALL_TYPE  ORIGIN_CALL  ORIGIN_STAND   TAXI_ID  \\\n",
       "0  1372636858620000589         C          NaN           NaN  20000589   \n",
       "1  1372637303620000596         B          NaN           7.0  20000596   \n",
       "2  1372636951620000320         C          NaN           NaN  20000320   \n",
       "3  1372636854620000520         C          NaN           NaN  20000520   \n",
       "4  1372637091620000337         C          NaN           NaN  20000337   \n",
       "\n",
       "    TIMESTAMP DAY_TYPE  MISSING_DATA  \\\n",
       "0  1372636858        A         False   \n",
       "1  1372637303        A         False   \n",
       "2  1372636951        A         False   \n",
       "3  1372636854        A         False   \n",
       "4  1372637091        A         False   \n",
       "\n",
       "                                            POLYLINE  \n",
       "0  [[-8.618643,41.141412],[-8.618499,41.141376],[...  \n",
       "1  [[-8.639847,41.159826],[-8.640351,41.159871],[...  \n",
       "2  [[-8.612964,41.140359],[-8.613378,41.14035],[-...  \n",
       "3  [[-8.574678,41.151951],[-8.574705,41.151942],[...  \n",
       "4  [[-8.645994,41.18049],[-8.645949,41.180517],[-...  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tr = pd.read_csv(\"train.csv\")\n",
    "df_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d08ade",
   "metadata": {},
   "source": [
    "### 3. Get Computed Time from POLYLINE\n",
    "\n",
    "Our goal is to predict the travel-time of the taxi, which can be derived from the POLYLINE length.\n",
    "\n",
    "Recall:\n",
    "\n",
    "```\n",
    "The travel time of the trip (the prediction target of this project) is defined as the (number of points-1) x 15 seconds. \n",
    "For example, a trip with 101 data points in POLYLINE has a length of (101-1) * 15 = 1500 seconds. Some trips have missing data points in POLYLINE, indicated by MISSING_DATA column, and it is part of the challenge how you utilize this knowledge.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "137aba93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Over every single \n",
    "def polyline_to_trip_duration(polyline):\n",
    "  return max(polyline.count(\"[\") - 1, 0) * 15\n",
    "\n",
    "# This code creates a new column, \"LEN\", in our dataframe. The value is\n",
    "# the (polyline_length - 1) * 15\n",
    "df_tr[\"LEN\"] = df_tr[\"POLYLINE\"].apply(polyline_to_trip_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c16cb37",
   "metadata": {},
   "source": [
    "### 4. Test and Extract the features: (Original Call + HR + WK + MON + TAXI_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "15b58b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern is found!\n"
     ]
    }
   ],
   "source": [
    "# Verify our guesses of the patterns of TAXI_ID such that all the IDs are in the form of \n",
    "# 20000xxx by substracting all the numbers by 20000000 and check if they are between the \n",
    "# range [0,1000).\n",
    "def TAXI_ID_pattern_checker(x):\n",
    "    # Test if the only last 3 digits of the TRIP_ID exhibit a pattern\n",
    "    for idx in range(len(x)):\n",
    "        if (x[idx]-20000000) < 0 or (x[idx]-20000000) >= 1000:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "if TAXI_ID_pattern_checker(df_tr[\"TAXI_ID\"]):\n",
    "    print(\"Pattern is found!\")\n",
    "\n",
    "# Note that the only last three digits of the TAXI_ID are nonzero.\n",
    "def parse_TAXI_ID(x):\n",
    "    return (x % pow(10,3)) \n",
    "\n",
    "df_tr[\"Unique_TAXI_ID\"] = df_tr[\"TAXI_ID\"].apply(parse_TAXI_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6abbca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def parse_time(x):\n",
    "  # We are using python's builtin datetime library\n",
    "  # https://docs.python.org/3/library/datetime.html#datetime.date.fromtimestamp\n",
    "\n",
    "  # Each x is essentially a 1 row, 1 column pandas Series\n",
    "  dt = datetime.fromtimestamp(x[\"TIMESTAMP\"])\n",
    "  return dt.year, dt.month, dt.day, dt.hour, dt.weekday()\n",
    "\n",
    "# Because we are assigning multiple values at a time, we need to \"expand\" our computed (year, month, day, hour, weekday) tuples on \n",
    "# the column axis, or axis 1\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html\n",
    "df_tr[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\"]] = df_tr[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a19787c",
   "metadata": {},
   "source": [
    "### 5. Data Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4e8c0452",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_threshold = 3\n",
    "\n",
    "type_A = df_tr[df_tr[\"CALL_TYPE\"]==\"A\"]\n",
    "\n",
    "mean, std = type_A[\"LEN\"].mean(), type_A[\"LEN\"].std()\n",
    "# \"Choose all data, where the trip length is less than 3 standard deviations away from the mean\"\n",
    "# This is to remove outliers. Otherwise, our plots would look very squished (since there are some\n",
    "# VERRRRRY long taxi trips in the dataset)\n",
    "df_trimmed_A = type_A[type_A[\"LEN\"] < mean + outlier_threshold * std]\n",
    "\n",
    "# Shuffle the data set to avoid clustering and improve the randomness to our data set\n",
    "df_trimmed_A = df_trimmed_A.sample(frac = 1)\n",
    "# Split our data set into two parts: 80% training set and 20% test set.\n",
    "train_set_A = df_trimmed_A[0:int(len(df_trimmed_A) * 0.8)][[\"Unique_TAXI_ID\",\"ORIGIN_CALL\",\"MON\",\"WK\",\"HR\",\"LEN\"]]\n",
    "val_set_A = df_trimmed_A[int(len(df_trimmed_A)*0.8):][[\"Unique_TAXI_ID\",\"ORIGIN_CALL\",\"MON\",\"WK\",\"HR\",\"LEN\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4d2fb2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def id_to_binary_vector(x):\n",
    "    vector_len = 24\n",
    "    result = []\n",
    "    \n",
    "    for idx in range(len(x)):\n",
    "        feature_vector = np.zeros(vector_len)\n",
    "        feature_id = int(x.iloc[idx])\n",
    "        length = len(list('{0:0b}'.format(feature_id)))\n",
    "        \n",
    "        if feature_id != 0:\n",
    "            for digit in list('{0:0b}'.format(feature_id)):\n",
    "                feature_vector[vector_len-length] = float(digit)\n",
    "                length = length - 1\n",
    "                \n",
    "        result.append(feature_vector)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def id_to_one_hot_vector(x):\n",
    "    result = []\n",
    "    \n",
    "    for idx in range(len(x)):\n",
    "        vector = [0]*24\n",
    "        vector[x.iloc[idx]] = 1\n",
    "        result.append(vector)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9ec4791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_A_MON = np.float32(id_to_one_hot_vector(train_set_A[\"MON\"]))\n",
    "val_data_A_MON = np.float32(id_to_one_hot_vector(val_set_A[\"MON\"]))\n",
    "\n",
    "train_data_A_WK = np.float32(id_to_one_hot_vector(train_set_A[\"WK\"]))\n",
    "val_data_A_WK = np.float32(id_to_one_hot_vector(val_set_A[\"WK\"]))\n",
    "\n",
    "train_data_A_HR = np.float32(id_to_one_hot_vector(train_set_A[\"HR\"]))\n",
    "val_data_A_HR = np.float32(id_to_one_hot_vector(val_set_A[\"HR\"]))\n",
    "\n",
    "train_data_A_TAXI_ID = np.float32(id_to_binary_vector(train_set_A[\"Unique_TAXI_ID\"]))\n",
    "val_data_A_TAXI_ID = np.float32(id_to_binary_vector(val_set_A[\"Unique_TAXI_ID\"]))\n",
    "\n",
    "train_data_A_ORIGIN_CALL = np.float32(id_to_binary_vector(train_set_A[\"ORIGIN_CALL\"]))\n",
    "val_data_A_ORIGIN_CALL = np.float32(id_to_binary_vector(val_set_A[\"ORIGIN_CALL\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8be0fb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_train_set_A = []\n",
    "result_val_set_A = []\n",
    "\n",
    "for idx in range(len(train_set_A[\"MON\"])):\n",
    "    result_train_set_A.append(np.column_stack((train_data_A_MON[idx],\n",
    "                                               train_data_A_WK[idx],\n",
    "                                               train_data_A_HR[idx],\n",
    "                                               train_data_A_TAXI_ID[idx],\n",
    "                                               train_data_A_ORIGIN_CALL[idx])))\n",
    "\n",
    "for idx in range(len(val_set_A[\"MON\"])):\n",
    "    result_val_set_A.append(np.column_stack((val_data_A_MON[idx],\n",
    "                                               val_data_A_WK[idx],\n",
    "                                               val_data_A_HR[idx],\n",
    "                                               val_data_A_TAXI_ID[idx],\n",
    "                                               val_data_A_ORIGIN_CALL[idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9fe7deb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in range(len(train_set_A[\"MON\"])):\n",
    "#     result_train_set_A.append(np.column_stack((train_data_A_MON[idx],\n",
    "#                                                train_data_A_WK[idx],\n",
    "#                                                train_data_A_HR[idx])))\n",
    "\n",
    "# for idx in range(len(val_set_A[\"MON\"])):\n",
    "#     result_val_set_A.append(np.column_stack((val_data_A_MON[idx],\n",
    "#                                                val_data_A_WK[idx],\n",
    "#                                                val_data_A_HR[idx])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1889d057",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label_A = train_set_A[\"LEN\"].tolist()\n",
    "train_label_max = max(train_label_A)\n",
    "train_label_min = min(train_label_A)\n",
    "train_label_A_normalize = [(val - train_label_min) / (train_label_max - train_label_min) for val in train_label_A]\n",
    "\n",
    "val_label_A = val_set_A[\"LEN\"].tolist()\n",
    "val_label_max = max(val_label_A)\n",
    "val_label_min = min(val_label_A)\n",
    "val_label_A_normalize = [(val -  val_label_min) / (val_label_max - val_label_min) for val in val_label_A]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "396ae63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bound = (50 - train_label_min) / (train_label_max - train_label_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "201c898a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.02178649237472767"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c0e291ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_A = torch.Tensor(result_train_set_A)\n",
    "val_data_A = torch.Tensor(result_val_set_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5266f438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data into tuples (data, label) so that \n",
    "# Dataloader in PyTorch can make use of them and we \n",
    "# can traverse each dataset with data + labels\n",
    "train_tuple_A = list(zip(train_data_A, train_label_A_normalize))\n",
    "val_tuple_A = list(zip(val_data_A, val_label_A_normalize))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d892fe6a",
   "metadata": {},
   "source": [
    "### 6. Conversion of Dataset to Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "7f17783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Dataset retrieves our dataset’s features and labels one sample at a time. While training a model, \n",
    "# we typically want to pass samples in “minibatches”, reshuffle the data at every epoch to reduce model overfitting, \n",
    "# and use Python’s multiprocessing to speed up data retrieval.\n",
    "#\n",
    "#\n",
    "# Typical Ways to iterate through the dataset: \n",
    "# iterator = iter(train_loader) sample = next(iterator)\n",
    "\n",
    "train_loader_A = torch.utils.data.DataLoader(train_tuple_A, batch_size=32,\n",
    "                                      shuffle=True, num_workers=2)\n",
    "\n",
    "val_loader_A = torch.utils.data.DataLoader(val_tuple_A, batch_size=32,\n",
    "                                      shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2b4f9dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device will determine whether to run the training on GPU or CPU.\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e66075",
   "metadata": {},
   "source": [
    "### 7. Building up a neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2d44c8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MLP_Classifier(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_Classifier, self).__init__() #Refers to the fact that this is a subclass of nn.Module and is inheriting all methods\n",
    "        \"\"\"\n",
    "        the __init__() method that defines the layers and other components\n",
    "        \"\"\" \n",
    "        self.model = torch.nn.Sequential( #an ordered container of modules\n",
    "            nn.Linear(24*5, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 32),\n",
    "            nn.Dropout(p = 0.2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )   \n",
    "        \n",
    "    def forward(self, x): #You never have to call model.forward(x)\n",
    "        \"\"\"\n",
    "        the forward function is where computatioin gets done\n",
    "        \"\"\"\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        \n",
    "        out = self.model(x)    \n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4e50255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP_Classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e6b563c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP_Classifier(\n",
       "  (model): Sequential(\n",
       "    (0): Linear(in_features=120, out_features=120, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=120, out_features=32, bias=True)\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): ReLU()\n",
       "    (5): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608b2b12",
   "metadata": {},
   "source": [
    "### 8. Define loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "73f2e4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9462dd",
   "metadata": {},
   "source": [
    "### 9. Test and Train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "dca34649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(train_loader, model, optimizer, loss_function):\n",
    "    losses = []\n",
    "    # get a batch of training data from the train_loader (DataLoader obj)\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "                    \n",
    "#         inputs = inputs.to(device)\n",
    "#         labels = labels.to(device)\n",
    "        \n",
    "        # make predictions for this batch\n",
    "        output = model(inputs)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = loss_function(output, labels.float())\n",
    "        \n",
    "        # Backpropagation\n",
    "        # zero out the gradients so that it will not accumulate through each iteration\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute the gradents with the backward call (backprop)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weight using gradient descent \n",
    "        optimizer.step()\n",
    "        \n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    return np.mean(losses)\n",
    "\n",
    "def eval_epoch(valid_loader, model, loss_function,bound):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    preds = []\n",
    "    trues = []\n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        for i, (x, t) in enumerate(valid_loader):\n",
    "            \n",
    "            # Compute prediction\n",
    "            y = model(x)\n",
    "            \n",
    "            prediction = y.data\n",
    "            \n",
    "            total += t.shape[0]\n",
    "            for idx in range(len(t)):\n",
    "                if abs(prediction[idx] - t[idx]) < bound:\n",
    "                    correct += 1\n",
    "            preds.append(y.data.numpy())\n",
    "            trues.append(t.data.numpy())\n",
    "            \n",
    "    return correct/total*100., np.concatenate(preds), np.concatenate(trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f413f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [01:57<37:19, 117.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train Loss: 135690.2868, Train Accuracy: 10.97%, Validation Accuracy: 10.85% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "total_epochs = 20\n",
    "train_accs, valid_accs = [], []\n",
    "max_acc = 0\n",
    "for epoch in tqdm(range(total_epochs)):\n",
    "    \n",
    "    model.train() # gradient tracking is on\n",
    "    \n",
    "    train_loss = train_epoch(train_loader_A, model, optimizer, criterion)\n",
    "    train_loss = train_loss * ((train_label_max - train_label_min)**2) # Normalzie the training loss back\n",
    "\n",
    "    model.eval() # we don't need gradients on to do reporting\n",
    "    \n",
    "    train_acc, train_preds, train_trues = eval_epoch(train_loader_A, model, criterion, train_bound)\n",
    "    valid_acc, valid_preds, valid_trues = eval_epoch(val_loader_A, model, criterion, train_bound)\n",
    "\n",
    "    train_accs.append(train_acc)\n",
    "    valid_accs.append(valid_acc)\n",
    " \n",
    "    print(f\"Epoch: {epoch+1}, Train Loss: {train_loss:>0.4f}, Train Accuracy: {train_acc:>0.2f}%, Validation Accuracy: {valid_acc:>0.2f}% \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77694deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(train_loader_A):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "    inputs, labels = data\n",
    "                    \n",
    "#         inputs = inputs.to(device)\n",
    "#         labels = labels.to(device)\n",
    "        \n",
    "        # make predictions for this batch\n",
    "    output = model(inputs)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4c35a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_normalize = [(val * (train_label_max - train_label_min) + train_label_min) for val in output]\n",
    "output_normalize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
