{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53897189",
   "metadata": {},
   "source": [
    "### 1. Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77dad5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a30a6ae",
   "metadata": {},
   "source": [
    "### 2. Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5c3ff45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TRIP_ID</th>\n",
       "      <th>CALL_TYPE</th>\n",
       "      <th>ORIGIN_CALL</th>\n",
       "      <th>ORIGIN_STAND</th>\n",
       "      <th>TAXI_ID</th>\n",
       "      <th>TIMESTAMP</th>\n",
       "      <th>DAY_TYPE</th>\n",
       "      <th>MISSING_DATA</th>\n",
       "      <th>POLYLINE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1372636858620000589</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000589</td>\n",
       "      <td>1372636858</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.618643,41.141412],[-8.618499,41.141376],[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1372637303620000596</td>\n",
       "      <td>B</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>20000596</td>\n",
       "      <td>1372637303</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.639847,41.159826],[-8.640351,41.159871],[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1372636951620000320</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000320</td>\n",
       "      <td>1372636951</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.612964,41.140359],[-8.613378,41.14035],[-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1372636854620000520</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000520</td>\n",
       "      <td>1372636854</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.574678,41.151951],[-8.574705,41.151942],[...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1372637091620000337</td>\n",
       "      <td>C</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20000337</td>\n",
       "      <td>1372637091</td>\n",
       "      <td>A</td>\n",
       "      <td>False</td>\n",
       "      <td>[[-8.645994,41.18049],[-8.645949,41.180517],[-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               TRIP_ID CALL_TYPE  ORIGIN_CALL  ORIGIN_STAND   TAXI_ID  \\\n",
       "0  1372636858620000589         C          NaN           NaN  20000589   \n",
       "1  1372637303620000596         B          NaN           7.0  20000596   \n",
       "2  1372636951620000320         C          NaN           NaN  20000320   \n",
       "3  1372636854620000520         C          NaN           NaN  20000520   \n",
       "4  1372637091620000337         C          NaN           NaN  20000337   \n",
       "\n",
       "    TIMESTAMP DAY_TYPE  MISSING_DATA  \\\n",
       "0  1372636858        A         False   \n",
       "1  1372637303        A         False   \n",
       "2  1372636951        A         False   \n",
       "3  1372636854        A         False   \n",
       "4  1372637091        A         False   \n",
       "\n",
       "                                            POLYLINE  \n",
       "0  [[-8.618643,41.141412],[-8.618499,41.141376],[...  \n",
       "1  [[-8.639847,41.159826],[-8.640351,41.159871],[...  \n",
       "2  [[-8.612964,41.140359],[-8.613378,41.14035],[-...  \n",
       "3  [[-8.574678,41.151951],[-8.574705,41.151942],[...  \n",
       "4  [[-8.645994,41.18049],[-8.645949,41.180517],[-...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tr = pd.read_csv(\"../data/train.csv\")\n",
    "df_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d08ade",
   "metadata": {},
   "source": [
    "### 3. Get Computed Time from POLYLINE\n",
    "\n",
    "Our goal is to predict the travel-time of the taxi, which can be derived from the POLYLINE length.\n",
    "\n",
    "Recall:\n",
    "\n",
    "```\n",
    "The travel time of the trip (the prediction target of this project) is defined as the (number of points-1) x 15 seconds. \n",
    "For example, a trip with 101 data points in POLYLINE has a length of (101-1) * 15 = 1500 seconds. Some trips have missing data points in POLYLINE, indicated by MISSING_DATA column, and it is part of the challenge how you utilize this knowledge.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "137aba93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Over every single \n",
    "def polyline_to_trip_duration(polyline):\n",
    "  return max(polyline.count(\"[\") - 1, 0) * 15\n",
    "\n",
    "# This code creates a new column, \"LEN\", in our dataframe. The value is\n",
    "# the (polyline_length - 1) * 15\n",
    "df_tr[\"LEN\"] = df_tr[\"POLYLINE\"].apply(polyline_to_trip_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c16cb37",
   "metadata": {},
   "source": [
    "### 4. Test and Extract the features: (Original Call + HR + WK + MON + TAXI_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15b58b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern is found!\n"
     ]
    }
   ],
   "source": [
    "# Verify our guesses of the patterns of TAXI_ID such that all the IDs are in the form of \n",
    "# 20000xxx by substracting all the numbers by 20000000 and check if they are between the \n",
    "# range [0,1000).\n",
    "def TAXI_ID_pattern_checker(x):\n",
    "    # Test if the only last 3 digits of the TRIP_ID exhibit a pattern\n",
    "    for idx in range(len(x)):\n",
    "        if (x[idx]-20000000) < 0 or (x[idx]-20000000) >= 1000:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "if TAXI_ID_pattern_checker(df_tr[\"TAXI_ID\"]):\n",
    "    print(\"Pattern is found!\")\n",
    "\n",
    "# Note that the only last three digits of the TAXI_ID are nonzero.\n",
    "def parse_TAXI_ID(x):\n",
    "    return (x % pow(10,3)) \n",
    "\n",
    "df_tr[\"Unique_TAXI_ID\"] = df_tr[\"TAXI_ID\"].apply(parse_TAXI_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6abbca66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "def parse_time(x):\n",
    "  # We are using python's builtin datetime library\n",
    "  # https://docs.python.org/3/library/datetime.html#datetime.date.fromtimestamp\n",
    "\n",
    "  # Each x is essentially a 1 row, 1 column pandas Series\n",
    "  dt = datetime.fromtimestamp(x[\"TIMESTAMP\"])\n",
    "  return dt.year, dt.month, dt.day, dt.hour, dt.weekday()\n",
    "\n",
    "# Because we are assigning multiple values at a time, we need to \"expand\" our computed (year, month, day, hour, weekday) tuples on \n",
    "# the column axis, or axis 1\n",
    "# https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html\n",
    "df_tr[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\"]] = df_tr[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "588cdb88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correlation coefficient between ORIGIN STAND and LEN is -0.0566367104027852\n",
      "The correlation coefficient between HR and LEN is 0.06702327616822985\n",
      "The correlation coefficient between WK and LEN is -0.06274757938021086\n",
      "The correlation coefficient between MON and LEN is 0.0087850062833805\n",
      "The correlation coefficient between TAXI_ID and LEN is 0.00992357768658932\n"
     ]
    }
   ],
   "source": [
    "outlier_threshold = 3\n",
    "\n",
    "# Trying to find relationship between \"ORIGIN_CALL\" and \"LEN\"\n",
    "type_B = df_tr[df_tr[\"CALL_TYPE\"]==\"B\"]\n",
    "\n",
    "mean, std = type_B[\"LEN\"].mean(), type_B[\"LEN\"].std()\n",
    "# \"Choose all data, where the trip length is less than 3 standard deviations away from the mean\"\n",
    "# This is to remove outliers. Otherwise, our plots would look very squished (since there are some\n",
    "# VERRRRRY long taxi trips in the dataset)\n",
    "df_trimmed_B = type_B[type_B[\"LEN\"] < mean + outlier_threshold * std]\n",
    "# df_trimmed_A = df_trimmed_A[df_trimmed_A?[\"ORIGIN_STAND\"]\n",
    "# plt.scatter(df_trimmed_A[\"ORIGIN_CALL\"], df_trimmed_A[\"LEN\"],s=5, alpha=0.5)\n",
    "# plt.xlabel(\"ORIGIN_CALL\")\n",
    "# plt.ylabel(\"LEN\")\n",
    "# plt.title(\"Relationship between ORIGIN_CALL and LEN\")\n",
    "# plt.show()\n",
    "\n",
    "print(\"The correlation coefficient between ORIGIN STAND and LEN is {}\".format(df_trimmed_B[\"ORIGIN_STAND\"].corr(df_trimmed_B[\"LEN\"])))\n",
    "print(\"The correlation coefficient between HR and LEN is {}\".format(df_trimmed_B[\"HR\"].corr(df_trimmed_B[\"LEN\"])))\n",
    "print(\"The correlation coefficient between WK and LEN is {}\".format(df_trimmed_B[\"WK\"].corr(df_trimmed_B[\"LEN\"])))\n",
    "print(\"The correlation coefficient between MON and LEN is {}\".format(df_trimmed_B[\"MON\"].corr(df_trimmed_B[\"LEN\"])))\n",
    "print(\"The correlation coefficient between TAXI_ID and LEN is {}\".format(df_trimmed_B[\"Unique_TAXI_ID\"].corr(df_trimmed_B[\"LEN\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a19787c",
   "metadata": {},
   "source": [
    "### 5. Data Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e8c0452",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_360/3533375885.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_features[\"ORIGIN_STAND\"] = all_features[\"ORIGIN_STAND\"].astype(str)\n",
      "/tmp/ipykernel_360/3533375885.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_features['WK'] = all_features['WK'].astype(str)\n",
      "/tmp/ipykernel_360/3533375885.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_features['HR'] = all_features['HR'].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预处理之后数据形状: (809308, 95)\n",
      "         ORIGIN_STAND_1.0  ORIGIN_STAND_10.0  ORIGIN_STAND_11.0  \\\n",
      "1                       0                  0                  0   \n",
      "15                      0                  0                  0   \n",
      "16                      0                  0                  0   \n",
      "23                      0                  0                  0   \n",
      "28                      0                  0                  0   \n",
      "...                   ...                ...                ...   \n",
      "1710654                 0                  0                  0   \n",
      "1710661                 0                  0                  0   \n",
      "1710662                 0                  0                  0   \n",
      "1710668                 0                  0                  0   \n",
      "1710669                 0                  0                  0   \n",
      "\n",
      "         ORIGIN_STAND_12.0  ORIGIN_STAND_13.0  ORIGIN_STAND_14.0  \\\n",
      "1                        0                  0                  0   \n",
      "15                       0                  1                  0   \n",
      "16                       0                  0                  0   \n",
      "23                       0                  0                  0   \n",
      "28                       0                  0                  0   \n",
      "...                    ...                ...                ...   \n",
      "1710654                  0                  0                  0   \n",
      "1710661                  0                  0                  0   \n",
      "1710662                  0                  0                  0   \n",
      "1710668                  1                  0                  0   \n",
      "1710669                  0                  0                  0   \n",
      "\n",
      "         ORIGIN_STAND_15.0  ORIGIN_STAND_16.0  ORIGIN_STAND_17.0  \\\n",
      "1                        0                  0                  0   \n",
      "15                       0                  0                  0   \n",
      "16                       0                  0                  0   \n",
      "23                       0                  0                  0   \n",
      "28                       0                  0                  0   \n",
      "...                    ...                ...                ...   \n",
      "1710654                  1                  0                  0   \n",
      "1710661                  1                  0                  0   \n",
      "1710662                  0                  0                  0   \n",
      "1710668                  0                  0                  0   \n",
      "1710669                  0                  0                  0   \n",
      "\n",
      "         ORIGIN_STAND_18.0  ...  HR_21  HR_22  HR_23  HR_3  HR_4  HR_5  HR_6  \\\n",
      "1                        0  ...      0      0      0     0     0     0     0   \n",
      "15                       0  ...      0      0      0     0     0     0     0   \n",
      "16                       0  ...      0      0      0     0     0     0     0   \n",
      "23                       0  ...      0      0      0     0     0     0     0   \n",
      "28                       0  ...      0      0      0     0     0     0     0   \n",
      "...                    ...  ...    ...    ...    ...   ...   ...   ...   ...   \n",
      "1710654                  0  ...      0      1      0     0     0     0     0   \n",
      "1710661                  0  ...      1      0      0     0     0     0     0   \n",
      "1710662                  0  ...      0      0      0     0     0     0     0   \n",
      "1710668                  0  ...      0      0      0     0     0     0     0   \n",
      "1710669                  0  ...      0      0      0     0     0     0     0   \n",
      "\n",
      "         HR_7  HR_8  HR_9  \n",
      "1           0     0     0  \n",
      "15          0     0     0  \n",
      "16          0     0     0  \n",
      "23          0     0     0  \n",
      "28          0     0     0  \n",
      "...       ...   ...   ...  \n",
      "1710654     0     0     0  \n",
      "1710661     0     0     0  \n",
      "1710662     0     0     0  \n",
      "1710668     0     0     0  \n",
      "1710669     0     0     0  \n",
      "\n",
      "[809308 rows x 95 columns]\n",
      "['ORIGIN_STAND_1.0', 'ORIGIN_STAND_10.0', 'ORIGIN_STAND_11.0', 'ORIGIN_STAND_12.0', 'ORIGIN_STAND_13.0', 'ORIGIN_STAND_14.0', 'ORIGIN_STAND_15.0', 'ORIGIN_STAND_16.0', 'ORIGIN_STAND_17.0', 'ORIGIN_STAND_18.0', 'ORIGIN_STAND_19.0', 'ORIGIN_STAND_2.0', 'ORIGIN_STAND_20.0', 'ORIGIN_STAND_21.0', 'ORIGIN_STAND_22.0', 'ORIGIN_STAND_23.0', 'ORIGIN_STAND_24.0', 'ORIGIN_STAND_25.0', 'ORIGIN_STAND_26.0', 'ORIGIN_STAND_27.0', 'ORIGIN_STAND_28.0', 'ORIGIN_STAND_29.0', 'ORIGIN_STAND_3.0', 'ORIGIN_STAND_30.0', 'ORIGIN_STAND_31.0', 'ORIGIN_STAND_32.0', 'ORIGIN_STAND_33.0', 'ORIGIN_STAND_34.0', 'ORIGIN_STAND_35.0', 'ORIGIN_STAND_36.0', 'ORIGIN_STAND_37.0', 'ORIGIN_STAND_38.0', 'ORIGIN_STAND_39.0', 'ORIGIN_STAND_4.0', 'ORIGIN_STAND_40.0', 'ORIGIN_STAND_41.0', 'ORIGIN_STAND_42.0', 'ORIGIN_STAND_43.0', 'ORIGIN_STAND_44.0', 'ORIGIN_STAND_45.0', 'ORIGIN_STAND_46.0', 'ORIGIN_STAND_47.0', 'ORIGIN_STAND_48.0', 'ORIGIN_STAND_49.0', 'ORIGIN_STAND_5.0', 'ORIGIN_STAND_50.0', 'ORIGIN_STAND_51.0', 'ORIGIN_STAND_52.0', 'ORIGIN_STAND_53.0', 'ORIGIN_STAND_54.0', 'ORIGIN_STAND_55.0', 'ORIGIN_STAND_56.0', 'ORIGIN_STAND_57.0', 'ORIGIN_STAND_58.0', 'ORIGIN_STAND_59.0', 'ORIGIN_STAND_6.0', 'ORIGIN_STAND_60.0', 'ORIGIN_STAND_61.0', 'ORIGIN_STAND_62.0', 'ORIGIN_STAND_63.0', 'ORIGIN_STAND_7.0', 'ORIGIN_STAND_8.0', 'ORIGIN_STAND_9.0', 'ORIGIN_STAND_nan', 'WK_0', 'WK_1', 'WK_2', 'WK_3', 'WK_4', 'WK_5', 'WK_6', 'HR_0', 'HR_1', 'HR_10', 'HR_11', 'HR_12', 'HR_13', 'HR_14', 'HR_15', 'HR_16', 'HR_17', 'HR_18', 'HR_19', 'HR_2', 'HR_20', 'HR_21', 'HR_22', 'HR_23', 'HR_3', 'HR_4', 'HR_5', 'HR_6', 'HR_7', 'HR_8', 'HR_9']\n"
     ]
    }
   ],
   "source": [
    "outlier_threshold = 3\n",
    "\n",
    "type_B = df_tr[df_tr[\"CALL_TYPE\"]==\"B\"]\n",
    "\n",
    "mean, std = type_B[\"LEN\"].mean(), type_B[\"LEN\"].std()\n",
    "# \"Choose all data, where the trip length is less than 3 standard deviations away from the mean\"\n",
    "# This is to remove outliers. Otherwise, our plots would look very squished (since there are some\n",
    "# VERRRRRY long taxi trips in the dataset)\n",
    "df_trimmed_B = type_B[type_B[\"LEN\"] < mean + outlier_threshold * std]\n",
    "\n",
    "# print(df_trimmed_A[\"Unique_TAXI_ID\"].mean())\n",
    "# print(df_trimmed_A[\"Unique_TAXI_ID\"].std())\n",
    "\n",
    "# print(df_trimmed_A[\"ORIGIN_STAND\"].mean())\n",
    "# print(df_trimmed_A[\"ORIGIN_STAND\"].std())\n",
    "\n",
    "\n",
    "all_features = df_trimmed_B[[\"ORIGIN_STAND\",\"WK\",\"HR\"]]\n",
    "\n",
    "# Get rid of row with nan in the ORIGIN_STAND column (or possibly other columns?)\n",
    "all_features.dropna()\n",
    "\n",
    "all_features[\"ORIGIN_STAND\"] = all_features[\"ORIGIN_STAND\"].astype(str)\n",
    "all_features['WK'] = all_features['WK'].astype(str)\n",
    "all_features['HR'] = all_features['HR'].astype(str)\n",
    "\n",
    "\n",
    "all_features = pd.get_dummies(all_features)\n",
    "print(f'预处理之后数据形状: {all_features.shape}')\n",
    "print(all_features)\n",
    "\n",
    "cols = list(all_features.columns.values)\n",
    "print(cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1889d057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集数据: torch.Size([647446, 95])\n",
      "训练集label: torch.Size([647446, 1])\n",
      "验证集数据: torch.Size([161862, 95])\n",
      "验证集label: torch.Size([161862, 1])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "label = df_trimmed_B[\"LEN\"]\n",
    "train_data = all_features\n",
    "\n",
    "train_features, val_features, train_labels, val_labels = train_test_split(\n",
    "    train_data, label, test_size=0.2, random_state=42)\n",
    "\n",
    "# 训练集\n",
    "train_features = torch.tensor(train_features.values, dtype=torch.float)\n",
    "# 验证集\n",
    "val_features = torch.tensor(val_features.values, dtype=torch.float)\n",
    "\n",
    "train_labels = torch.tensor(train_labels.values, dtype=torch.float)\n",
    "train_labels = train_labels.unsqueeze(1) \n",
    "\n",
    "\n",
    "val_labels = torch.tensor(val_labels.values, dtype=torch.float)\n",
    "val_labels = val_labels.unsqueeze(1)\n",
    "\n",
    "\n",
    "print(f'训练集数据: {train_features.shape}')\n",
    "print(f'训练集label: {train_labels.shape}')\n",
    "print(f'验证集数据: {val_features.shape}')\n",
    "print(f'验证集label: {val_labels.shape}')\n",
    "\n",
    "print(val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0e291ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataset:\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx, :], self.label[idx]\n",
    "\n",
    "train_dataset = myDataset(train_features, train_labels)\n",
    "val_dataset = myDataset(val_features, val_labels)\n",
    "\n",
    "# 变为迭代器\n",
    "train_iter = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "val_iter = torch.utils.data.DataLoader(dataset=val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d892fe6a",
   "metadata": {},
   "source": [
    "### 6. Conversion of Dataset to Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5266f438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 初始化权重\n",
    "def _weight_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Conv2d):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "    elif isinstance(m, nn.BatchNorm1d):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "# 网络\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(95, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.fc5 = nn.Linear(128, 64)  # New layer: fc5 -> fc6\n",
    "        self.fc6 = nn.Linear(64, 32)   # New layer: fc6 -> fc7\n",
    "        self.fc7 = nn.Linear(32, 1)    # New layer: fc7 -> output\n",
    "        self.apply(_weight_init)\n",
    "        self.apply(_weight_init) # 初始化参数\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = F.relu(self.fc5(x))\n",
    "        x = F.relu(self.fc6(x))\n",
    "        x = self.fc7(x)\n",
    "        return x\n",
    "\n",
    "# 使用rmse作为自定义得分函数，这也是比赛的判定标准\n",
    "def custom_score(y_true, y_pred):\n",
    "#     rmse = mean_squared_error(np.log1p(y_true), np.log1p(y_pred), squared=False)\n",
    "#     return rmse\n",
    "    return math.sqrt(np.mean((np.array(y_pred)-np.array(y_true))*(np.array(y_pred)-np.array(y_true))))\n",
    "\n",
    "\n",
    "net = Net()\n",
    "# net.load_state_dict(torch.load('model.pth'))\n",
    "criterion = torch.nn.MSELoss() # 损失函数为MSE\n",
    "net = net.to(device) # 将网络和损失函数转化为GPU或CPU\n",
    "criterion = criterion.to(device)\n",
    "optimizer = torch.optim.Adam(params=net.parameters(), lr=0.005, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f17783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这是训练函数，分为train和val\n",
    "# train时前向传播后向更新参数\n",
    "# val时只计算损失函数\n",
    "def train(net, data_iter, phase, criterion, optimizer=None):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    mean_loss = []\n",
    "    is_grad = True if phase == 'train' else False\n",
    "    with torch.set_grad_enabled(is_grad):\n",
    "        net.train()\n",
    "        for step, (X, y) in enumerate(data_iter):\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            out = net(X)\n",
    "            loss = criterion(out, y) # 计算损失\n",
    "            mean_loss.append(loss.item())\n",
    "            \n",
    "            if phase == 'train':\n",
    "                optimizer.zero_grad() # optimizer 0\n",
    "                loss.backward() # back propragation\n",
    "                optimizer.step() # update the paramters\n",
    "\n",
    "            # 将每一个step的结果加入列表，最后统一生产这一个epoch的指标  \n",
    "            # 添加预测值和真实类标签\n",
    "            y_pred.extend(out.detach().cpu().squeeze().numpy().tolist())\n",
    "            y_true.extend(y.detach().cpu().squeeze().numpy().tolist())\n",
    "\n",
    "    # 全量样本的rmse和平均loss\n",
    "    rmse = custom_score(y_true, y_pred)\n",
    "    mean_loss = np.mean(mean_loss)\n",
    "    # 保留4位小数\n",
    "    rmse = np.round(rmse, 4)\n",
    "    mean_loss = np.round(mean_loss, 4)\n",
    "    return mean_loss, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2b4f9dd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-23 06:46:33.883027 开始训练...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [02:14<20:12, 134.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[40mEpoch: 0 Train loss: 104408.3968 Val loss: 103747.5526 Train score: 323.1221 Val score: 322.099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [04:26<17:43, 132.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[40mEpoch: 1 Train loss: 104243.5754 Val loss: 103510.3224 Train score: 322.866 Val score: 321.7309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [06:35<15:17, 131.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[40mEpoch: 2 Train loss: 104108.5294 Val loss: 103861.8333 Train score: 322.6586 Val score: 322.2747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [08:47<13:09, 131.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[40mEpoch: 3 Train loss: 103995.3488 Val loss: 103654.6672 Train score: 322.4819 Val score: 321.9546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [10:59<10:59, 131.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[40mEpoch: 4 Train loss: 103925.12 Val loss: 103394.0489 Train score: 322.3719 Val score: 321.5497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [13:09<08:44, 131.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[40mEpoch: 5 Train loss: 103886.6908 Val loss: 103291.6127 Train score: 322.3139 Val score: 321.3906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [17:22<04:17, 128.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[40mEpoch: 7 Train loss: 103755.173 Val loss: 103579.233 Train score: 322.1104 Val score: 321.8371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [19:34<02:09, 129.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[40mEpoch: 8 Train loss: 103681.0612 Val loss: 103369.5014 Train score: 321.9951 Val score: 321.5101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [21:48<00:00, 130.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[40mEpoch: 9 Train loss: 103674.6041 Val loss: 103243.746 Train score: 321.986 Val score: 321.3159\n",
      "2023-05-23 07:08:22.425679 训练结束...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from colorama import Fore, Back\n",
    "import math\n",
    "\n",
    "epochs = 100\n",
    "loss_list_B = []\n",
    "print(f'{datetime.now()} 开始训练...')\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    train_mean_loss, train_score = train(net=net, \n",
    "                                         data_iter=train_iter, \n",
    "                                         phase='train', \n",
    "                                         criterion=criterion, \n",
    "                                         optimizer=optimizer)\n",
    "    \n",
    "    val_mean_loss, val_score = train(net=net, \n",
    "                                     data_iter=train_iter, \n",
    "                                     phase='val', \n",
    "                                     criterion=criterion, \n",
    "                                     optimizer=None)\n",
    "    print(Fore.CYAN + Back.BLACK, end='')\n",
    "    tqdm.write(f'Epoch: {epoch} Train loss: {train_mean_loss} Val loss: {val_mean_loss}', end=' ')\n",
    "    tqdm.write(f'Train score: {train_score} Val score: {val_score}')\n",
    "    loss_list_B.append(train_score)\n",
    "\n",
    "print(f'{datetime.now()} 训练结束...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c23de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('loss_list_B.txt', 'w') as file:\n",
    "    file.write(','.join(str(element) for element in loss_list_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728ee6e8",
   "metadata": {},
   "source": [
    "### 8. Test set validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ac6bc610",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_360/1882698335.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_features_test[\"ORIGIN_STAND\"] = all_features_test[\"ORIGIN_STAND\"].astype(str)\n",
      "/tmp/ipykernel_360/1882698335.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_features_test['WK'] = all_features_test['WK'].astype(str)\n",
      "/tmp/ipykernel_360/1882698335.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  all_features_test['HR'] = all_features_test['HR'].astype(str)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE loss against test set is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "577.8904117612195"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tr_test = pd.read_csv(\"../test/test.csv\")\n",
    "df_tr_test.head()\n",
    "\n",
    "df_tr_test[\"LEN\"] = df_tr_test[\"POLYLINE\"].apply(polyline_to_trip_duration)\n",
    "df_tr_test[[\"YR\", \"MON\", \"DAY\", \"HR\", \"WK\"]] = df_tr_test[[\"TIMESTAMP\"]].apply(parse_time, axis=1, result_type=\"expand\")\n",
    "\n",
    "df_tr_test = df_tr_test[df_tr_test[\"CALL_TYPE\"]==\"B\"]\n",
    "\n",
    "all_features_test = df_trimmed_B[[\"ORIGIN_STAND\",\"WK\",\"HR\"]]\n",
    "\n",
    "all_features_test[\"ORIGIN_STAND\"] = all_features_test[\"ORIGIN_STAND\"].astype(str)\n",
    "all_features_test['WK'] = all_features_test['WK'].astype(str)\n",
    "all_features_test['HR'] = all_features_test['HR'].astype(str)\n",
    "\n",
    "all_features_test = pd.get_dummies(all_features_test)\n",
    "\n",
    "missing_columns = set(all_features.columns)-set(all_features_test.columns)\n",
    "\n",
    "for column in missing_columns:\n",
    "    all_features_test[column] = 0\n",
    "\n",
    "cols = list(all_features_test.columns.values)\n",
    "\n",
    "prediction = net(torch.tensor(all_features_test.values, dtype=torch.float).to(device))\n",
    "\n",
    "print(\"The RMSE loss against test set is:\")\n",
    "custom_score(prediction.tolist(),df_tr_test[\"LEN\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d876ae4",
   "metadata": {},
   "source": [
    "### 9. Model Saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f0a6e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Optimizer's state_dict:\")\n",
    "# for var_name in optimizer.state_dict():\n",
    "#     print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
    "# val_features = val_features.to(device)\n",
    "torch.save(net.state_dict(), '../model/modelB.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
